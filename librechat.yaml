# For more information, see the Configuration Guide:
# https://www.librechat.ai/docs/configuration/librechat_yaml

# Configuration version (required)
version: 1.2.1

# Cache settings: Set to true to enable caching
cache: true

# File storage configuration
# Single strategy for all file types (legacy format, still supported)
# fileStrategy: "s3"

# Granular file storage strategies (new format - recommended)
# Allows different storage strategies for different file types
# fileStrategy:
#   avatar: "s3"        # Storage for user/agent avatar images
#   image: "firebase"   # Storage for uploaded images in chats
#   document: "local"   # Storage for document uploads (PDFs, text files, etc.)

# Available strategies: "local", "s3", "firebase"
# If not specified, defaults to "local" for all file types
# You can mix and match strategies based on your needs:
# - Use S3 for avatars for fast global access
# - Use Firebase for images with automatic optimization
# - Use local storage for documents for privacy/compliance

# Custom interface configuration
interface:
  # customWelcome: uses dynamic time-based greeting with user's name
  fileSearch: true
  privacyPolicy:
    externalUrl: 'https://ground0.io/privacy'
    openNewTab: true

  termsOfService:
    externalUrl: 'https://ground0.io/terms'
    openNewTab: true
    modalAcceptance: true
    modalTitle: 'Terms of Service for Ground Zero'
    modalContent: |
      # Terms and Conditions for Ground Zero

      *Effective Date: February 9, 2026*

      Welcome to Ground Zero, a multi-model AI platform operated by Ground Zero GmbH, available at https://ground0.io. These Terms of Service ("Terms") govern your use of our platform and the services we offer. By accessing or using the Platform, you agree to be bound by these Terms and our Privacy Policy, accessible at https://ground0.io/privacy.

      ## 1. Service Description

      Ground Zero provides access to multiple AI models through a credit-based system. Users may interact with various AI assistants for chat, document generation, image creation, and other AI-powered services. Ground Zero GmbH reserves the right to modify, suspend, or discontinue any aspect of the service at any time.

      ## 2. User Accounts and Credits

      To use the Platform, you must create an account with accurate information. Upon registration, you receive a one-time allocation of free credits. Additional credits may be purchased through subscription plans or credit packs. Credits are non-transferable and non-refundable unless required by applicable law. Unused credits from credit packs expire after 90 days.

      ## 3. User Data

      We collect personal data, such as your name, email address, and payment information, as described in our Privacy Policy. This information is collected to provide and improve our services, process transactions, and communicate with you. Your conversations with AI models may be stored to provide service continuity.

      ## 4. Acceptable Use

      You agree to use the Platform only for lawful purposes. You shall not use the Platform to generate content that is illegal, harmful, threatening, abusive, harassing, defamatory, or otherwise objectionable. You shall not attempt to circumvent credit limits, abuse free tier allocations, or exploit the service in any unauthorized manner.

      ## 5. Intellectual Property

      Content you create using the Platform belongs to you, subject to the terms of the underlying AI model providers. Ground Zero GmbH retains all rights to the Platform itself, including its design, code, and branding.

      ## 6. Limitation of Liability

      AI-generated content may contain errors or inaccuracies. Ground Zero GmbH provides the service "as is" without warranties of any kind. We shall not be liable for any damages arising from your use of the Platform or reliance on AI-generated content.

      ## 7. Governing Law

      These Terms shall be governed by and construed in accordance with the laws of the Federal Republic of Germany. The courts of Munich, Germany shall have exclusive jurisdiction over any disputes arising from these Terms.

      ## 8. Changes to the Terms

      We reserve the right to modify these Terms at any time. We will notify users of any changes by email. Your continued use of the Platform after such changes have been notified will constitute your consent to such changes.

      ## 9. Contact Information

      If you have any questions about these Terms, please contact us at:

      Ground Zero GmbH
      Lautenschlagerstrasse 10
      80999 Munich, Germany
      CEO: Mustapha Nada
      Email: support@ground0.io

      By using the Platform, you acknowledge that you have read these Terms of Service and agree to be bound by them.

  endpointsMenu: true
  modelSelect: true
  parameters: true
  sidePanel: false
  presets: true
  prompts:
    use: true
    share: false
    public: false
  bookmarks: true
  multiConvo: true
  agents:
    use: true
    share: false
    public: false
  peoplePicker:
    users: true
    groups: true
    roles: true
  marketplace:
    use: false
  fileCitations: true
  # MCP Servers configuration example
  # mcpServers:
    # Controls user permissions for MCP (Model Context Protocol) server management
    # - use: Allow users to use configured MCP servers
    # - create: Allow users to create and manage new MCP servers
    # - share: Allow users to share MCP servers with other users
    # - public: Allow users to share MCP servers publicly (with everyone)
     
    # Creation / edit MCP server config Dialog config example
    # trustCheckbox:
    #   label:
    #     en: 'I understand and I want to continue'
    #     de: 'Ich verstehe und möchte fortfahren'
    #     de-DE: 'Ich verstehe und möchte fortfahren' # You can narrow translation to regions like (de-DE or de-CH)
    #   subLabel:
    #     en: |
    #       Librechat hasn't reviewed this MCP server. Attackers may attempt to steal your data or trick the model into taking unintended actions, including destroying data. <a href="https://google.de" target="_blank"><strong>Learn more.</strong></a>
    #     de: |
    #       LibreChat hat diesen MCP-Server nicht überprüft. Angreifer könnten versuchen, Ihre Daten zu stehlen oder das Modell zu unbeabsichtigten Aktionen zu verleiten, einschließlich der Zerstörung von Daten. <a href="https://google.de" target="_blank"><strong>Mehr erfahren.</strong></a>

  # Temporary chat retention period in hours (default: 720, min: 1, max: 8760)
  # temporaryChatRetention: 1

# Example Cloudflare turnstile (optional)
#turnstile:
#  siteKey: "your-site-key-here"
#  options:
#    language: "auto"    # "auto" or an ISO 639-1 language code (e.g. en)
#    size: "normal"      # Options: "normal", "compact", "flexible", or "invisible"

# Example Registration Object Structure (optional)
registration:
  socialLogins: ['github', 'google', 'discord', 'openid', 'facebook', 'apple', 'saml']
  # allowedDomains:
  # - "gmail.com"

balance:
  enabled: true
  startBalance: 100
  freeModelThreshold: 1.0
  freeTierLimit: 15
  autoRefillEnabled: false
  modelTiers:
    # Free: 0 credits (rate-limited 15/hr, always available for signed-up users)
    gemini-2.0-flash: 0
    gemini-2.0-flash-lite: 0
    gemini-2.5-flash-lite: 0
    gemini-1.5-flash: 0
    mistral-small: 0
    mistral-nemo: 0
    llama3: 0
    qwen2.5: 0
    qwen3: 0
    gemma: 0
    # Budget: 1 credit
    llama3-3-70b: 1
    llama3.3: 1
    mixtral: 1
    # Standard: 2 credits
    gemini-2.5-flash: 2
    gpt-4o-mini: 2
    gpt-4.1-mini: 2
    gpt-4.1-nano: 2
    mistral-large: 2
    # Premium: 3 credits
    gpt-4o: 3
    gpt-4.1: 3
    gpt-5.2: 3
    gpt-5.1: 3
    gpt-5: 3
    gpt-5-mini: 2
    claude-sonnet-4: 3
    claude-4.5-sonnet: 3
    claude-3-5-sonnet: 3
    claude-3.5-sonnet: 3
    claude-3-7-sonnet: 3
    claude-3.7-sonnet: 3
    claude-3.5-haiku: 3
    claude-3-5-haiku: 3
    claude-haiku-4-5: 3
    gemini-2.5-pro: 3
    grok-3: 3
    command-r-plus: 3
    # Reasoning: 5 credits
    o1: 5
    o1-mini: 5
    o3: 5
    o3-mini: 5
    o4-mini: 5
    deepseek-reasoner: 5
    deepseek-r1: 5
    # Deep/Premium: 10 credits
    claude-opus-4: 10
    claude-opus-4-5: 10
    claude-opus-4-6: 10
    gpt-4.5: 10
    gpt-5-pro: 10
    gemini-3: 10

# Example Transactions settings
# Controls whether to save transaction records to the database
# Default is true (enabled)
#transactions:
#  enabled: false
# Note: If balance.enabled is true, transactions will always be enabled
# regardless of this setting to ensure balance tracking works correctly

# speech:
#   tts:
#     openai:
#       url: ''
#       apiKey: '${TTS_API_KEY}'
#       model: ''
#       voices: ['']

#
#   stt:
#     openai:
#       url: ''
#       apiKey: '${STT_API_KEY}'
#       model: ''

# rateLimits:
#   fileUploads:
#     ipMax: 100
#     ipWindowInMinutes: 60  # Rate limit window for file uploads per IP
#     userMax: 50
#     userWindowInMinutes: 60  # Rate limit window for file uploads per user
#   conversationsImport:
#     ipMax: 100
#     ipWindowInMinutes: 60  # Rate limit window for conversation imports per IP
#     userMax: 50
#     userWindowInMinutes: 60  # Rate limit window for conversation imports per user

# Agent Actions domain restrictions (OpenAPI spec validation)
# SECURITY: If not configured, SSRF targets are blocked (localhost, private IPs, .internal/.local TLDs).
# To allow internal targets, you MUST explicitly add them to allowedDomains.
# Supports wildcards: '*.example.com' and protocol/port restrictions: 'https://api.example.com:8443'
actions:
  allowedDomains:
    - 'swapi.dev'
    - 'librechat.ai'
    - 'google.com'
    # - 'http://10.225.26.25:7894'  # Internal IP with protocol/port (uncomment if needed)

# MCP Server domain restrictions for remote transports (SSE, WebSocket, HTTP)
# SECURITY: If not configured, SSRF targets are blocked (localhost, private IPs, .internal/.local TLDs).
# To allow internal targets like host.docker.internal, you MUST explicitly add them to allowedDomains.
# Supports wildcards: '*.example.com' matches 'api.example.com', 'staging.example.com', etc.
# Supports protocol/port restrictions: 'https://api.example.com:8443' restricts to specific protocol/port.
# mcpSettings:
#   allowedDomains:
#     - 'host.docker.internal'    # Docker host access (required for Docker setups)
#     - 'localhost'               # Local development
#     - '*.example.com'           # Wildcard subdomain
#     - 'https://secure.api.com'  # Protocol-restricted
#     - 'http://internal:8080'    # Protocol and port restricted

# Example MCP Servers Object Structure
# mcpServers:
#   everything:
#     # type: sse # type can optionally be omitted
#     url: http://localhost:3001/sse
#     timeout: 60000  # 1 minute timeout for this server, this is the default timeout for MCP servers.
#   puppeteer:
#     type: stdio
#     command: npx
#     args:
#       - -y
#       - "@modelcontextprotocol/server-puppeteer"
#     timeout: 300000  # 5 minutes timeout for this server
#   filesystem:
#     # type: stdio
#     command: npx
#     args:
#       - -y
#       - "@modelcontextprotocol/server-filesystem"
#       - /home/user/LibreChat/
#     iconPath: /home/user/LibreChat/client/public/assets/logo.svg
#   mcp-obsidian:
#     command: npx
#     args:
#       - -y
#       - "mcp-obsidian"
#       - /path/to/obsidian/vault

# Definition of custom endpoints
endpoints:
  # assistants:
  #   disableBuilder: false # Disable Assistants Builder Interface by setting to `true`
  #   pollIntervalMs: 3000  # Polling interval for checking assistant updates
  #   timeoutMs: 180000  # Timeout for assistant operations
  #   # Should only be one or the other, either `supportedIds` or `excludedIds`
  #   supportedIds: ["asst_supportedAssistantId1", "asst_supportedAssistantId2"]
  #   # excludedIds: ["asst_excludedAssistantId"]
  #   # Only show assistants that the user created or that were created externally (e.g. in Assistants playground).
  #   # privateAssistants: false # Does not work with `supportedIds` or `excludedIds`
  #   # (optional) Models that support retrieval, will default to latest known OpenAI models that support the feature
  #   retrievalModels: ["gpt-4-turbo-preview"]
  #   # (optional) Assistant Capabilities available to all users. Omit the ones you wish to exclude. Defaults to list below.
  #   capabilities: ["code_interpreter", "retrieval", "actions", "tools", "image_vision"]
  # agents:
  #   # (optional) Default recursion depth for agents, defaults to 25
  #   recursionLimit: 50
  #   # (optional) Max recursion depth for agents, defaults to 25
  #   maxRecursionLimit: 100
  #   # (optional) Disable the builder interface for agents
  #   disableBuilder: false
  #   # (optional) Maximum total citations to include in agent responses, defaults to 30
  #   maxCitations: 30
  #   # (optional) Maximum citations per file to include in agent responses, defaults to 7
  #   maxCitationsPerFile: 7
  #   # (optional) Minimum relevance score for sources to be included in responses, defaults to 0.45 (45% relevance threshold)
  #   # Set to 0.0 to show all sources (no filtering), or higher like 0.7 for stricter filtering
  #   minRelevanceScore: 0.45
  #   # (optional) Agent Capabilities available to all users. Omit the ones you wish to exclude. Defaults to list below.
  #   capabilities: ["execute_code", "file_search", "actions", "tools"]

  # Anthropic endpoint configuration with Vertex AI support
  # Use this to run Anthropic Claude models through Google Cloud Vertex AI
  # anthropic:
  #   # (optional) Stream rate limiting in milliseconds
  #   streamRate: 20
  #   # (optional) Title model for conversation titles
  #   titleModel: claude-3.5-haiku  # Use the visible model name (key from models config)
  #
  #   # Vertex AI Configuration - enables running Claude models via Google Cloud
  #   # This is similar to Azure OpenAI but for Anthropic models on Google Cloud
  #   # Vertex AI is automatically enabled when this config section is present
  #   vertex:
  #     # Vertex AI region (optional, defaults to 'us-east5')
  #     # Available regions: us-east5, us-central1, europe-west1, europe-west4, asia-southeast1
  #     region: "us-east5"
  #     # Path to Google service account key file (optional)
  #     # If not specified, uses GOOGLE_SERVICE_KEY_FILE env var or default path (api/data/auth.json)
  #     # The project_id is automatically extracted from the service key file
  #     # serviceKeyFile: "/path/to/service-account.json"
  #     # Google Cloud Project ID (optional) - auto-detected from service key file
  #     # Only specify if you need to override the project_id in your service key
  #     # projectId: "${VERTEX_PROJECT_ID}"
  #
  #     # ============================================================================
  #     # Model Configuration - Set Visible Model Names and Deployment Mappings
  #     # Similar to Azure OpenAI model naming pattern
  #     # ============================================================================
  #
  #     # Option 1: Simple array (legacy format - model name = deployment name)
  #     # Use this if you want the technical model IDs to show in the UI
  #     # models:
  #     #   - "claude-sonnet-4-20250514"
  #     #   - "claude-3-7-sonnet-20250219"
  #     #   - "claude-3-5-sonnet-v2@20241022"
  #     #   - "claude-3-5-haiku@20241022"
  #
  #     # Option 2: Object format with custom visible names (RECOMMENDED)
  #     # The key is the visible model name shown in the UI (can be any name you want)
  #     # The deploymentName is the actual Vertex AI model ID used for API calls
  #     # You can use friendly names (avoid spaces for cleaner YAML) or technical IDs as keys
  #     models:
  #       claude-opus-4.5:
  #         deploymentName: claude-opus-4-5@20251101
  #       claude-sonnet-4:
  #         deploymentName: claude-sonnet-4-20250514
  #       claude-3.7-sonnet:
  #         deploymentName: claude-3-7-sonnet-20250219
  #       claude-3.5-sonnet:
  #         deploymentName: claude-3-5-sonnet-v2@20241022
  #       claude-3.5-haiku:
  #         deploymentName: claude-3-5-haiku@20241022
  #
  #     # Option 3: Mixed format with default deploymentName
  #     # Set a default deploymentName and use boolean values for models
  #     # deploymentName: claude-sonnet-4-20250514
  #     # models:
  #     #   claude-sonnet-4: true  # Will use the default deploymentName
  #     #   claude-3.5-haiku:
  #     #     deploymentName: claude-3-5-haiku@20241022  # Override for this model

  custom:
    # Groq Example
    - name: 'groq'
      apiKey: '${GROQ_API_KEY}'
      baseURL: 'https://api.groq.com/openai/v1/'
      models:
        default:
          - 'llama3-70b-8192'
          - 'llama3-8b-8192'
          - 'llama2-70b-4096'
          - 'mixtral-8x7b-32768'
          - 'gemma-7b-it'
        fetch: false
      titleConvo: true
      titleModel: 'mixtral-8x7b-32768'
      modelDisplayLabel: 'groq'

    # Mistral AI Example
    - name: 'Mistral' # Unique name for the endpoint
      # For `apiKey` and `baseURL`, you can use environment variables that you define.
      # recommended environment variables:
      apiKey: '${MISTRAL_API_KEY}'
      baseURL: 'https://api.mistral.ai/v1'

      # Models configuration
      models:
        # List of default models to use. At least one value is required.
        default: ['mistral-tiny', 'mistral-small', 'mistral-medium']
        # Fetch option: Set to true to fetch models from API.
        fetch: true # Defaults to false.

      # Optional configurations

      # Title Conversation setting
      titleConvo: true # Set to true to enable title conversation

      # Title Method: Choose between "completion" or "functions".
      # titleMethod: "completion"  # Defaults to "completion" if omitted.

      # Title Model: Specify the model to use for titles.
      titleModel: 'mistral-tiny' # Defaults to "gpt-3.5-turbo" if omitted.

      # Summarize setting: Set to true to enable summarization.
      # summarize: false

      # Summary Model: Specify the model to use if summarization is enabled.
      # summaryModel: "mistral-tiny"  # Defaults to "gpt-3.5-turbo" if omitted.

      # The label displayed for the AI model in messages.
      modelDisplayLabel: 'Mistral' # Default is "AI" when not set.

      # Add additional parameters to the request. Default params will be overwritten.
      # addParams:
      # safe_prompt: true # This field is specific to Mistral AI: https://docs.mistral.ai/api/

      # Drop Default params parameters from the request. See default params in guide linked below.
      # NOTE: For Mistral, it is necessary to drop the following parameters or you will encounter a 422 Error:
      dropParams: ['stop', 'user', 'frequency_penalty', 'presence_penalty']

    # OpenRouter Example
    - name: 'OpenRouter'
      # For `apiKey` and `baseURL`, you can use environment variables that you define.
      # recommended environment variables:
      apiKey: '${OPENROUTER_KEY}'
      baseURL: 'https://openrouter.ai/api/v1'
      headers:
        x-librechat-body-parentmessageid: '{{LIBRECHAT_BODY_PARENTMESSAGEID}}'
      models:
        default: ['meta-llama/llama-3-70b-instruct']
        fetch: true
      titleConvo: true
      titleModel: 'meta-llama/llama-3-70b-instruct'
      # Recommended: Drop the stop parameter from the request as Openrouter models use a variety of stop tokens.
      dropParams: ['stop']
      modelDisplayLabel: 'OpenRouter'

    # Helicone Example
    - name: 'Helicone'
      # For `apiKey` and `baseURL`, you can use environment variables that you define.
      # recommended environment variables:
      apiKey: '${HELICONE_KEY}'
      baseURL: 'https://ai-gateway.helicone.ai'
      headers:
        x-librechat-body-parentmessageid: '{{LIBRECHAT_BODY_PARENTMESSAGEID}}'
      models:
        default:
          ['gpt-4o-mini', 'claude-4.5-sonnet', 'llama-3.1-8b-instruct', 'gemini-2.5-flash-lite']
        fetch: true
      titleConvo: true
      titleModel: 'gpt-4o-mini'
      modelDisplayLabel: 'Helicone'
      iconURL: https://marketing-assets-helicone.s3.us-west-2.amazonaws.com/helicone.png

    # Portkey AI Example
    - name: 'Portkey'
      apiKey: 'dummy'
      baseURL: 'https://api.portkey.ai/v1'
      headers:
        x-portkey-api-key: '${PORTKEY_API_KEY}'
        x-portkey-virtual-key: '${PORTKEY_OPENAI_VIRTUAL_KEY}'
      models:
        default: ['gpt-4o-mini', 'gpt-4o', 'chatgpt-4o-latest']
        fetch: true
      titleConvo: true
      titleModel: 'current_model'
      summarize: false
      summaryModel: 'current_model'
      modelDisplayLabel: 'Portkey'
      iconURL: https://images.crunchbase.com/image/upload/c_pad,f_auto,q_auto:eco,dpr_1/rjqy7ghvjoiu4cd1xjbf

  # AWS Bedrock Example
  # Note: Bedrock endpoint is configured via environment variables
  # bedrock:
  #   # Models Configuration
  #   # Specify which models are available (equivalent to BEDROCK_AWS_MODELS env variable)
  #   models:
  #     - "anthropic.claude-3-7-sonnet-20250219-v1:0"
  #     - "anthropic.claude-3-5-sonnet-20241022-v2:0"
  #
  #   # Inference Profiles Configuration
  #   # Maps model IDs to their inference profile ARNs
  #   # IMPORTANT: The model ID (key) MUST be a valid AWS Bedrock model ID that you've added to the models list above
  #   # The ARN (value) is the inference profile you wish to map to for that model
  #   # Both the model ID and ARN are sent to AWS - the model ID for validation/metadata, the ARN for routing
  #   inferenceProfiles:
  #     "us.anthropic.claude-sonnet-4-20250514-v1:0": "${BEDROCK_INFERENCE_PROFILE_CLAUDE_SONNET}"
  #     "anthropic.claude-3-7-sonnet-20250219-v1:0": "arn:aws:bedrock:us-west-2:123456789012:application-inference-profile/abc123"
  #
  #   # Guardrail Configuration
  #   guardrailConfig:
  #     guardrailIdentifier: "your-guardrail-id"
  #     guardrailVersion: "1"
  #
  #     # Trace behavior for debugging (optional)
  #     # - "enabled": Include basic trace information about guardrail assessments
  #     # - "enabled_full": Include comprehensive trace details (recommended for debugging)
  #     # - "disabled": No trace information (default)
  #     # Trace output is logged to application log files for compliance auditing
  #     trace: "enabled"
# Example modelSpecs configuration showing grouping options
# The 'group' field organizes model specs in the UI selector:
# - If 'group' matches an endpoint name (e.g., "openAI", "groq"), the spec appears nested under that endpoint
# - If 'group' is a custom name (doesn't match any endpoint), it creates a separate collapsible section
# - If 'group' is omitted, the spec appears as a standalone item at the top level
#
# The 'groupIcon' field sets an icon for custom groups:
# - Only needs to be set on one spec per group (first one is used)
# - Can be a URL or a built-in endpoint key (e.g., "openAI", "anthropic", "groq")
# modelSpecs:
#   list:
#     # Example 1: Nested under an endpoint (grouped with openAI endpoint)
#     - name: "gpt-4o"
#       label: "GPT-4 Optimized"
#       description: "Most capable GPT-4 model with multimodal support"
#       group: "openAI"  # String value matching the endpoint name
#       preset:
#         endpoint: "openAI"
#         model: "gpt-4o"
#
#     # Example 2: Nested under a custom endpoint (grouped with groq endpoint)
#     - name: "llama3-70b-8192"
#       label: "Llama 3 70B"
#       description: "Fastest inference available - great for quick responses"
#       group: "groq"  # String value matching your custom endpoint name from endpoints.custom
#       preset:
#         endpoint: "groq"
#         model: "llama3-70b-8192"
#
#     # Example 3: Custom group with icon (creates a separate collapsible section)
#     - name: "coding-assistant"
#       label: "Coding Assistant"
#       description: "Specialized for coding tasks"
#       group: "my-assistants"  # Custom string - doesn't match any endpoint, so creates its own group
#       groupIcon: "https://example.com/icons/assistants.png"  # Icon URL for the group
#       preset:
#         endpoint: "openAI"
#         model: "gpt-4o"
#         instructions: "You are an expert coding assistant..."
#         temperature: 0.3
#
#     - name: "writing-assistant"
#       label: "Writing Assistant"
#       description: "Specialized for creative writing"
#       group: "my-assistants"  # Same custom group name - both specs appear in same section
#       # No need to set groupIcon again - the first spec's icon is used
#       preset:
#         endpoint: "anthropic"
#         model: "claude-sonnet-4"
#         instructions: "You are a creative writing expert..."
#
#     # Example 4: Custom group using built-in icon key
#     - name: "fast-models"
#       label: "Fast Response Model"
#       group: "Fast Models"
#       groupIcon: "groq"  # Uses the built-in Groq icon
#       preset:
#         endpoint: "groq"
#         model: "llama3-8b-8192"
#
#     # Example 5: Standalone (no group - appears at top level)
#     - name: "general-assistant"
#       label: "General Assistant"
#       description: "General purpose assistant"
#       # No 'group' field - appears as standalone item at top level (not nested)
#       preset:
#         endpoint: "openAI"
#         model: "gpt-4o-mini"

fileConfig:
  endpoints:
    default:
      fileLimit: 20
      fileSizeLimit: 30  # 30 MB per file
      totalSizeLimit: 100  # 100 MB total per request
      supportedMimeTypes:
        # Images
        - "image/.*"
        # Audio
        - "audio/.*"
        # Video
        - "video/.*"
        # Documents
        - "application/pdf"
        - "text/.*"
        - "application/json"
        - "application/vnd.openxmlformats-officedocument.*"
        - "application/vnd.ms-excel"
        - "application/msword"
  serverFileSizeLimit: 100  # Global server file size limit in MB
  avatarSizeLimit: 2  # Limit for user avatar image size in MB
# # See the Custom Configuration Guide for more information on Assistants Config:
# # https://www.librechat.ai/docs/configuration/librechat_yaml/object_structure/assistants_endpoint

# Web Search Configuration (optional)
# webSearch:
#   # Jina Reranking Configuration
#   jinaApiKey: '${JINA_API_KEY}'  # Your Jina API key
#   jinaApiUrl: '${JINA_API_URL}'  # Custom Jina API URL (optional, defaults to https://api.jina.ai/v1/rerank)
#   # Other rerankers
#   cohereApiKey: '${COHERE_API_KEY}'
#   # Search providers
#   serperApiKey: '${SERPER_API_KEY}'
#   searxngInstanceUrl: '${SEARXNG_INSTANCE_URL}'
#   searxngApiKey: '${SEARXNG_API_KEY}'
#   # Content scrapers
#   firecrawlApiKey: '${FIRECRAWL_API_KEY}'
#   firecrawlApiUrl: '${FIRECRAWL_API_URL}'

# Memory configuration for user memories
# memory:
#   # (optional) Disable memory functionality
#   disabled: false
#   # (optional) Restrict memory keys to specific values to limit memory storage and improve consistency
#   validKeys: ["preferences", "work_info", "personal_info", "skills", "interests", "context"]
#   # (optional) Maximum token limit for memory storage (not yet implemented for token counting)
#   tokenLimit: 10000
#   # (optional) Enable personalization features (defaults to true if memory is configured)
#   # When false, users will not see the Personalization tab in settings
#   personalize: true
#   # Memory agent configuration - either use an existing agent by ID or define inline
#   agent:
#     # Option 1: Use existing agent by ID
#     id: "your-memory-agent-id"
#     # Option 2: Define agent inline
#     # provider: "openai"
#     # model: "gpt-4o-mini"
#     # instructions: "You are a memory management assistant. Store and manage user information accurately."
#     # model_parameters:
#     #   temperature: 0.1
