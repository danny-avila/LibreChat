version: 1.3.1
#cache: true

# Automatic Memory - AI remembers important info across chats


interface:
  modelSelect: false
  agents: true
  memories: true                  # Show Memories in sidebar
  endpointsMenu: false            # Hide endpoints menu since we only use agents

endpoints:
  custom:
    - name: "Vicktoria"
      type: "custom"
      baseURL: ${RUNPOD_URI}
      apiKey: ${RUNPOD_API_KEY}

      # Make requests vLLM-compatible
      dropParams:
        - "user"
        - "frequency_penalty"
        - "presence_penalty"
        - "top_logprobs"
        - "logit_bias"
        - "response_format"

      addParams:
        stream: false  # enable (true) later once debugged

      models:
        default: ["google/gemma-3-27b-it"]
        fetch: true

      titleConvo: true
      titleModel: "current_model"
      titleMessageRole: "user"
      
      summarize: false
      summaryModel: "current_model"
      modelDisplayLabel: "Vicktoria"


# Memory configuration
memory:
  disabled: true
  personalize: false
  tokenLimit: 3000
  messageWindowSize: 5
  validKeys:
    - "user_preferences"
    - "conversation_context"
    - "learned_facts"
    - "personal_information"
  agent:
    provider: "Vicktoria"
    model: "google/gemma-3-27b-it"
    instructions: ${MEMORY_AGENT_INSTRUCTIONS}


